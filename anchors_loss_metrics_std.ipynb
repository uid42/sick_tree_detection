{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from exp import nb_resnet_ssd_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "#from exp import nb_bbox_hw_statistics\n",
    "from detect_symbol.exp import nb_databunch\n",
    "from detect_symbol.exp import nb_init_model\n",
    "from detect_symbol.exp import nb_resnet_ssd\n",
    "from detect_symbol.exp import nb_anchors_loss_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from IPython.core import debugger as idb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neibs(idx, grids = (49, 49), dis = 1):\n",
    "    '''\n",
    "    找到某个anchor周围相邻的anchors的下标里列表。距离默认1。\n",
    "    这个任务中只有第一层的grids参与，所以只需要第一次的grids的尺寸。\n",
    "    anchor也是1对1的。\n",
    "    参数：\n",
    "        idx：目标anchor在grid anchors(get_grid_anchors返回的gvs)列表中的下标\n",
    "        grids: 尺寸\n",
    "        dis：邻居的距离\n",
    "    返回值：\n",
    "        邻居的下标列表\n",
    "    '''\n",
    "    gh, gw = grids\n",
    "    x = idx // gh\n",
    "    y = idx % gw\n",
    "    ret = []\n",
    "    for nx in range(x - dis, x + dis + 1):\n",
    "        for ny in range(y - dis, y + dis + 1):\n",
    "            if nx >= 0 and ny >= 0 and nx < gw and ny < gh \\\n",
    "                    and not(nx == x and ny == y):\n",
    "                nidx = ny * gw + nx\n",
    "                #print(x, y, nx, ny, nidx)\n",
    "                ret += [nidx]\n",
    "    return ret      \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#定义一个新的GridAnchor_Functions，主要是修改get_scroe_hits\n",
    "class GridAnchor_Funcs_std(nb_anchors_loss_metrics.GridAnchor_Funcs):\n",
    "    def __init__(self, fig_hw, grids, device):\n",
    "        anchors = [[(1, 1)]]\n",
    "        gvs,ghs,gws,avs,ahs,aws = nb_anchors_loss_metrics.get_grids_anchors( \\\n",
    "                    fig_hw, grids, anchors)\n",
    "        self.grids = grids\n",
    "        super().__init__(gvs, avs, device)\n",
    "        \n",
    "    def get_scores_hits(self, gt_bboxs): \n",
    "        # ground truch bbox center x,y\n",
    "        gt_cx = gt_bboxs[:,[0,2]].mean(-1)\n",
    "        gt_cy = gt_bboxs[:,[1,3]].mean(-1)\n",
    "\n",
    "        # 判断目标bbox的中心落在哪个cell内\n",
    "        hits = ((gt_cx[:,None] >= self.gvs[:,0][None]) &\n",
    "                (gt_cx[:,None] <  self.gvs[:,2][None]) &\n",
    "                (gt_cy[:,None] >= self.gvs[:,1][None]) &\n",
    "                (gt_cy[:,None] <  self.gvs[:,3][None]))\n",
    "        \n",
    "        # 在的就是1，不在就是0\n",
    "        scores = iou(gt_bboxs[:,None,:], fxdAncVs)\n",
    "        \n",
    "        return scores,hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### zip as function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_ga1():\n",
    "    return get_grids_anchors(fig_hw = (776,776), grids = [(49,49)]\n",
    "                             , anchors = [[(1,1)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### total loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def yolo_L_std(pred_batch, *gt_batch, conf_th=0.5,\n",
    "           lambda_cent=1, lambda_pconf=1, lambda_nconf=1, lambda_clas=1, lambda_hw=1, clas_weights=None, gaf):\n",
    "    '''\n",
    "    与detect_symbol里面的yolo_L相比的区别是：\n",
    "        不计算宽高方面的损失\n",
    "        neg_idx要去掉find_neibs返回的discard列表\n",
    "    \n",
    "    \n",
    "    clas_weights: \n",
    "    为了解决数据集的imbalance问题，一种方法是在dataloader中使用WeightedRandomSampler，但是这种方法不适用于目标检测问题。\n",
    "    因为，（1）目标检测的label不是一个简单的数值（2）目标检测问题的一张图片可能包括不同类别的多个目标。\n",
    "    所以为了解决目标检测问题中的imbalance问题，我们的方法是在损失函数中使用权重。\n",
    "    为各类别分配权重，各目标对应的损失乘以该目标所属类别的权重。\n",
    "    默认为None，即不使用权重。\n",
    "    若设置非None，则clas_weights应该是一个一维tensor，其长度等于数据集的类别数。\n",
    "    若设置为全1，则相当于不使用权重。\n",
    "    合理的设置应保证所有元素之和等于数据集的类别数，否则相当于对损失函数的整体做了缩放。\n",
    "    '''\n",
    "    clas_loss = 0\n",
    "    cent_loss = 0\n",
    "    pconf_loss = 0\n",
    "    nconf_loss = 0\n",
    "    hw_loss = 0\n",
    "    pos_cnt = 0\n",
    "    neg_cnt = 0\n",
    "    #import pdb; pdb.set_trace()\n",
    "    for pred_txy,pred_conf,pred_clas,pred_thw,gt_bboxs,gt_clas in zip(*pred_batch, *gt_batch):\n",
    "        keep = get_y(gt_bboxs)\n",
    "        if keep.numel()==0: continue\n",
    "          \n",
    "        gt_bboxs = gt_bboxs[keep]\n",
    "        gt_clas = gt_clas[keep]\n",
    "        \n",
    "        gt_bboxs = (gt_bboxs + 1) / 2\n",
    "        gt_clas = gt_clas - 1 # the databunch add a 'background' class to classes[0], but we don't want that,so gt_clas-1\n",
    "        \n",
    "        if clas_weights is not None: ws = clas_weights[gt_clas]\n",
    "        else: ws = None\n",
    "        \n",
    "        scores,hits = gaf.get_scores_hits(gt_bboxs)\n",
    "        idx = idx_fromScoresHits(scores,hits)\n",
    "        \n",
    "        # classification loss\n",
    "        pred_clas = pred_clas[idx]\n",
    "        clas_loss += F.cross_entropy(pred_clas, gt_clas, weight=clas_weights, reduction='sum')\n",
    "        \n",
    "        # bbox center loss\n",
    "        gt_t = gaf.b2t(gt_bboxs,idx,eps=1)\n",
    "        pred_txy = pred_txy[idx]\n",
    "        if ws is not None:\n",
    "            cent_loss += ((gt_t[...,:2]-pred_txy)*ws[...,None]).abs().sum()\n",
    "        else:\n",
    "            cent_loss += (gt_t[...,:2]-pred_txy).abs().sum()\n",
    "        \n",
    "        # bbox height and width loss\n",
    "        '''\n",
    "        pred_thw = pred_thw[idx]\n",
    "        if ws is not None: \n",
    "            hw_loss += ((gt_t[...,2:]-pred_thw)*ws[...,None]).abs().sum()\n",
    "        else: \n",
    "            hw_loss += (gt_t[...,2:]-pred_thw).abs().sum()\n",
    "        '''\n",
    "        \n",
    "        # positive confidence loss\n",
    "        conf_pos = pred_conf[idx]\n",
    "        if ws is not None: \n",
    "            pconf_loss += F.binary_cross_entropy_with_logits(conf_pos,torch.ones_like(conf_pos),weight=ws[...,None],reduction='sum')\n",
    "        else: \n",
    "            pconf_loss += F.binary_cross_entropy_with_logits(conf_pos,torch.ones_like(conf_pos),reduction='sum')\n",
    "        \n",
    "        # negative conficence loss\n",
    "        scores[range(0,len(idx)),idx] = conf_th + 1 # 强制责任anchor的得分好于threshold\n",
    "        tmp = scores>conf_th # 判断各得分是否好于threshold\n",
    "        tmp = tmp.max(dim=0)[0] # 各anchor是否有任意一个好于threshold的得分\n",
    "        neg_idx = torch.where(tmp==0)[0] # 如果没有，该anchor是negative anchor\n",
    "        conf_neg = pred_conf[negn_idx]\n",
    "        nconf_loss += F.binary_cross_entropy_with_logits(conf_neg,torch.zeros_like(conf_neg),reduction='sum')\n",
    "        \n",
    "        pos_cnt += len(idx)\n",
    "        neg_cnt += len(neg_idx)\n",
    "        \n",
    "    clas_loss  = lambda_clas  * clas_loss  /pos_cnt\n",
    "    cent_loss  = lambda_cent  * cent_loss  /pos_cnt\n",
    "    pconf_loss = lambda_pconf * pconf_loss /pos_cnt\n",
    "    nconf_loss = lambda_nconf * nconf_loss /neg_cnt\n",
    "    ###hw_loss    = lambda_hw    * hw_loss   /pos_cnt\n",
    "    \n",
    "    return clas_loss + cent_loss + pconf_loss + nconf_loss### + hw_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_grid_anchors的生成顺序\n",
    "grids = [(49,49),(25,25),(13,13)]\n",
    "anchors = [[(22,17),(22,37),(43,17),(43,37)],\n",
    "            [(43,77),(83,37),(83,77)],\n",
    "            [(83,162),(162,77),(162,162)]]\n",
    "t = [(x, y, gx, gy, ax, ay) for (gx,gy),ancs in zip(grids,anchors)\n",
    "                                for y in range(gy)\n",
    "                                for x in range(gx)\n",
    "                                for ax,ay in ancs]\n",
    "print(t[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试find_neibs\n",
    "neibs = find_neibs(100, (49, 49))\n",
    "\n",
    "gvs,ghs,gws,avs,ahs,aws = nb_anchors_loss_metrics.get_grids_anchors(fig_hw \\\n",
    "                                , grids, anchors)\n",
    "print('邻居', neibs)\n",
    "for idx in neibs:\n",
    "    print(idx, anchors)\n",
    "    print(avs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#距离之间的事情可以不考虑？先按照resnet18来一个\n",
    "# 获取 grids 和 anchors\n",
    "gvs,ghs,gws,avs,ahs,aws = nb_anchors_loss_metrics.get_grids_anchors(\n",
    "                                            fig_hw = (776,776),\n",
    "                                            grids = [(49,49)],\n",
    "                                            anchors = [[(1,1)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_hw = (784,784)\n",
    "# grids = [(49,49),(25,25),(13,13)]\n",
    "# anchors = [[(22,17),(22,37),(43,17),(43,37)],\n",
    "#                                          [(43,77),(83,37),(83,77)],\n",
    "#                                          [(83,162),(162,77),(162,162)]]\n",
    "#for (gx,gy),ancs in zip(grids,anchors) : \n",
    "#        print(gx,gy,ancs)\n",
    "        \n",
    "#at = [[gx, gy, ancs, y, x]  for (gx,gy),ancs in zip(grids,anchors) for y in range(gy) for x in range(gx)]    \n",
    "#print(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这些变量的尺寸\n",
    "gvs.shape, ghs.shape, gws.shape, avs.shape, ahs.shape, aws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "49*49*1  #只有一个anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds = './data/ds_20200429'\n",
    "#data = nb_databunch.get_databunch(data_root=ds, bs=4, device=device, cache=False) \n",
    "# get databunch\n",
    "data = nb_databunch.get_databunch(data_root = '../detect_symbol/data/ds_20200429', bs=4,device=device)\n",
    "x,y = data.one_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model and load init parameters\n",
    "\n",
    "model = nb_resnet_ssd_std.get_resnet18_1ssd_std()\n",
    "#model.load_state_dict(torch.load('./models/resnet18_ssd_init.pth'));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the statistics of layer outputs\n",
    "#nb_init_model.show_layer_stats(model,x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred a batch\n",
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find idx of the responsible anchor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择一个bbox\n",
    "samp = 1 # samp: sample, get one bbox from this sample\n",
    "bb = (y[0][samp][-1]+1)/2\n",
    "bb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaf = GridAnchor_Funcs_std(fig_hw = (776,776)\n",
    "                         , grids = [(49,49)]\n",
    "                         , device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 测试 get_scores_hits 和 idx_fromScoresHits\n",
    "scores,hits= gaf.get_scores_hits(bb[None])\n",
    "idx = idx_fromScoresHits(scores,hits)\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用另一种方法与之对照验证\n",
    "test_idx = test_getIdx(bb.clone())\n",
    "test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert idx.item()==test_idx.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# get GridAnchor_Funcs object\n",
    "gvs,_,_,avs,_,_ = get_ga433()\n",
    "gaf = GridAnchor_Funcs(gvs,avs,torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一个简单的clas_weights，你可以更改它的值来试验\n",
    "clas_weights = torch.ones(16)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classification loss\n",
    "clas_loss = clas_L(pred,*y,gaf=gaf, clas_weights=clas_weights)\n",
    "clas_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bbox center loss\n",
    "cent_loss = cent_L(pred,*y,gaf=gaf, clas_weights=clas_weights)\n",
    "cent_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# positive confidence loss\n",
    "pconf_loss = pConf_L(pred,*y,gaf=gaf, clas_weights=clas_weights)\n",
    "pconf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# negative confidence loss\n",
    "nconf_loss = nConf_L(pred,*y,gaf=gaf)\n",
    "nconf_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# bbox height and width loss\n",
    "hw_loss = hw_L(pred,*y,gaf=gaf, clas_weights=clas_weights)\n",
    "hw_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total loss\n",
    "total_loss = yolo_L(pred,*y, clas_weights=clas_weights,gaf=gaf)\n",
    "total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (clas_loss+cent_loss+pconf_loss+hw_loss+nconf_loss)==total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_acc(pred,*y,gaf=gaf), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clas = len(data.train_ds.y.classes)-1\n",
    "print(1/n_clas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bbox center difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cent_d(pred,*y,gaf=gaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((0.5/49)*49*49+(0.5/25)*25*25+(0.5/13)*13*13)/(49*49+25*25+13*13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bbox height and width ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_r(pred,*y,gaf=gaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个值是否合理？ \n",
    "\n",
    "下面我仅考虑高度 h 来分析，宽度 w 与之是同样的情况。  \n",
    "  \n",
    "根据定义，$b_h=p_h\\cdot e^{t_h}$，即 $log(\\frac{b_h}{p_h})=t_h$，其中 $t_h$ 是初始化模型的输出，属于标准正态分布。  \n",
    "  \n",
    "其中 $p_h$ 是 anchor 高度，因为我们在设计 anchor 时选择了 $p_h$ 位于 $o_h$（表示anchor所负责的那部分目标框的宽度）的分布的中心（对数坐标下），即$log(\\frac{o_h}{p_h})$是均值为0的随机分布，我们假设这个随机分布也是正态分布，标准差为$\\sigma_o$（小于1，因为我们设计 anchor 时，它所负责的目标框的宽度的范围约为\\[-1.4,+1.4\\]（对数坐标），那么可以粗略估计其标准差在0.5左右）。从正太分布的对称性上看，$log(\\frac{p_h}{o_h})=-log(\\frac{o_h}{p_h})$属于同样的分布。\n",
    "\n",
    "则$log(\\frac{b_h}{o_h})=log(\\frac{b_h}{p_h})+log(\\frac{p_h}{o_h})$应是均值为0，标准差为$\\sqrt{1+\\sigma_o^2}\\approx\\sqrt{1.25}\\approx1.1$的正态分布。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们令 $a=log(\\frac{b_h}{o_h})$ 属于 $N(0,1.1)$，下面生成随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.normal(0,1.1,size=(1,1000)).squeeze()\n",
    "a.mean(), a.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对照代码，我们生成的 a 对应 `(pred_hw/gt_hw).log()` 部分，所以"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logR = a.abs().sum()\n",
    "logR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为我们这里仅考虑了 h，所以相当于 `cnt` 不需要 `*2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(logR/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "得到的值与调用`hw_r()`的结果是接近的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!python notebook2script.py --fname 'anchors_loss_metrics.ipynb' --outputDir './exp/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
