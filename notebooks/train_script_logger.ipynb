{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basic_train import LearnerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.callbacks.general_sched import *\n",
    "from fastai.callback import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from IPython.core import debugger as idb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from FLAI.detect_symbol.exp import databunch\n",
    "from FLAI.detect_symbol.exp import resnet_ssd as resnet_ssd_detsym\n",
    "from FLAI.detect_symbol.exp import init_model as init_model_detsym\n",
    "from FLAI.detect_symbol.exp import anchors_loss_metrics as anchors_loss_metrics_detsym\n",
    "from FLAI.detect_symbol.exp import optimizer as optimizer_detsym\n",
    "from FLAI.detect_symbol.exp import tensorboard_callback as tensorboard_callback_detsym\n",
    "from FLAI.detect_symbol.exp import scheduling_train as scheduling_train_detsym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import sys\n",
    "if '..' not in sys.path:\n",
    "    sys.path.append('..')\n",
    "from exp import resnet_ssd\n",
    "from exp import anchors_loss_metrics\n",
    "from exp import init_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from torch.nn import Sequential, ModuleList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.basic_train import Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.torch_core import bn_types,bias_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os,shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.callbacks import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "from fastai.callbacks.tracker import SaveModelCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def txt_write(fh, i, opt, lr, path, csv_fname):\n",
    "    fh.write('===================================\\n')\n",
    "    fh.write(f'run_{i}\\n')\n",
    "    fh.write('-----------------------------------\\n')\n",
    "    fh.write(f'--opt_func: {opt}\\n')\n",
    "    fh.write(f'--lr: {lr}\\n')\n",
    "    fh.write(f'--csv_log: {path}/{csv_fname}.csv\\n')\n",
    "    fh.write(f'--best model: {path}/models/run_{i}.pth\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def multi_train(get_learn, epoch_len, epochs, opts, lrs, checkpoints, tb_log_root,autoSave=True):\n",
    "    '''\n",
    "    可以从checkpoint继续训练，为了保证训练连续性，需要手动设置lr与checkpoint保存时一致。\n",
    "    '''\n",
    "    # 清理tensorboard log dir\n",
    "    if os.path.exists(tb_log_root): shutil.rmtree(tb_log_root)\n",
    "    os.mkdir(tb_log_root)\n",
    "    \n",
    "    if not os.path.exists('./run_log/'): os.mkdir('./run_log/')\n",
    "    txtlog = open('./run_log/log.txt',mode='w')\n",
    "    for i,(opt,lr,checkpoint) in enumerate(zip(opts,lrs,checkpoints)):\n",
    "        # create a learner\n",
    "        learn = get_learn()\n",
    "        \n",
    "        # set optimizer\n",
    "        learn.opt_func = opt\n",
    "        \n",
    "        # load checkpoint\n",
    "        if checkpoint is not None:\n",
    "            with open(checkpoint,'rb') as f:\n",
    "                learn.load(f)\n",
    "        \n",
    "        # 在txt log中记录\n",
    "        csv_log_dir = f'csv_log/'\n",
    "        if not os.path.exists(learn.path/csv_log_dir): os.mkdir(learn.path/csv_log_dir)\n",
    "        csv_fname = csv_log_dir+f'run_{i}'\n",
    "        txt_write(txtlog,i,opt,lr,learn.path,csv_fname)\n",
    "        \n",
    "        callbacks = []\n",
    "        # get csvlogger callback\n",
    "        csvLog = CSVLogger(learn,filename=csv_fname)\n",
    "        callbacks += [csvLog]\n",
    "        \n",
    "        if autoSave:\n",
    "            # savemodel callback\n",
    "            autoSave = SaveModelCallback(learn,monitor='valid_loss',mode='min',every='improvement',name=f'run_{i}')\n",
    "            callbacks += [autoSave]\n",
    "        \n",
    "        # get tensorboard callback\n",
    "        tbCb = get_tbCb(learn,tb_log_root+f'run_{i}')\n",
    "        callbacks += [tbCb]\n",
    "        \n",
    "        # train\n",
    "        fit(learn=learn, epoch_len=epoch_len, epochs=epochs, lr=lr, callbacks=callbacks)\n",
    "        \n",
    "    txtlog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def split_model(model):\n",
    "#     idb.set_trace()\n",
    "    group0 = ModuleList()\n",
    "    group1 = ModuleList()\n",
    "    \n",
    "    pretrained_layers = Sequential(model.conv1, model.bn1, model.res_blocks[:4])\n",
    "#     noPretrain_layers = Sequential(model.res_blocks[4], model.neck_blocks, model.head_block)\n",
    "    noPretrain_layers = Sequential(model.neck_blocks, model.head_block)\n",
    "    \n",
    "    #把pretrained layers分作batchnorm部分（放在group1），和非batchnorm部分（放在group0）\n",
    "    for m in pretrained_layers.modules():\n",
    "        if isinstance(m,bn_types): group1.append(m)\n",
    "        elif isinstance(m,bias_types): group0.append(m)\n",
    "            \n",
    "    #把非pretrain的层放到group1\n",
    "    for m in noPretrain_layers.children():\n",
    "        group1.append(m)\n",
    "    \n",
    "    return [group0, group1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "# 设置device\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "device_ids = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "clas_cnts = [11191, 712, 1362, 224, 8710, 1212, 1139, 8686, 857, 2176, 6175, 1869, 14794, 1435, 13628, 9618, 1462]\n",
    "weights = anchors_loss_metrics_detsym.get_clasWeights(clas_cnts,10)\n",
    "weights = tensor(weights).float().to(device)\n",
    "# weights = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def get_learn_detectsym_17clas(data,gaf,clas_weights=weights):\n",
    "    '''\n",
    "    用的符号检测的17个类别的数据集\n",
    "    '''\n",
    "    # create model\n",
    "    model = resnet_ssd.get_resnet18_1ssd(num_classes = 17)\n",
    "    model.load_state_dict(torch.load('./models/pretrained_res18_1ssd.pth'));\n",
    "    \n",
    "    # create learner\n",
    "    learn = Learner(data,model)\n",
    "    \n",
    "    # split model\n",
    "    learn.layer_groups = split_model(learn.model)\n",
    "    \n",
    "    # set multi-gpu\n",
    "    if data.device.type=='cuda':\n",
    "        learn.model = torch.nn.DataParallel(learn.model,device_ids=device_ids)#device_ids=[0,1,2,3,4,5])\n",
    "        \n",
    "    # set loss func\n",
    "    learn.loss_func = partial(anchors_loss_metrics.yolo_L, gaf=gaf, conf_th=1, clas_weights=clas_weights, lambda_nconf=10)\n",
    "    \n",
    "    # 添加metrics\n",
    "    learn.metrics += [partial(anchors_loss_metrics.clas_L,   gaf=gaf, clas_weights=clas_weights)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics.cent_L,   gaf=gaf, clas_weights=clas_weights)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics.pConf_L,  gaf=gaf, clas_weights=clas_weights)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics.nConf_L,  gaf=gaf, conf_th=1)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics.clas_acc, gaf=gaf)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics.cent_d,   gaf=gaf)]\n",
    "    \n",
    "    return learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_resnet18_1ssd_dsmodel():\n",
    "    model = resnet_ssd_detsym.ResNetIsh_1SSD(block=torchvision.models.resnet.BasicBlock,\n",
    "                   layers=[2,2,2],\n",
    "                   chs=[64,128,256],\n",
    "                   strides=[1,2,2],\n",
    "                   pred_layerIds=[2],\n",
    "                   num_anchors=1,\n",
    "                   neck_block=resnet_ssd_detsym.cnv1x1_bn_relu,\n",
    "                   head_chin=256,\n",
    "                   head_block=resnet_ssd_detsym.ssd_block,\n",
    "                   num_classes=17)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#之前用没有宽高信息的病树检测的网络在符号检测的数据集上训练完毕后效果很好\n",
    "#这里再试一下带上宽高信息的\n",
    "def get_learn_detectsym_17clas_dsmodel(data,gaf,clas_weights=weights):\n",
    "    model = get_resnet18_1ssd_dsmodel()\n",
    "    \n",
    "    model.load_state_dict(torch.load('models/ds_pretrained_res18_1ssd.pth'));    \n",
    "    # create learner\n",
    "    learn = Learner(data,model)\n",
    "    \n",
    "    # split model\n",
    "    learn.layer_groups = split_model(learn.model)\n",
    "    \n",
    "    # set multi-gpu\n",
    "    if data.device.type=='cuda':\n",
    "        learn.model = torch.nn.DataParallel(learn.model,device_ids=[0,1])\n",
    "        \n",
    "    # set loss func\n",
    "    learn.loss_func = partial(anchors_loss_metrics_detsym.yolo_L, gaf=gaf, conf_th=1, clas_weights=clas_weights, lambda_nconf=10)\n",
    "    \n",
    "    # 添加metrics\n",
    "    learn.metrics += [partial(anchors_loss_metrics_detsym.clas_L,   gaf=gaf, clas_weights=clas_weights)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics_detsym.cent_L,   gaf=gaf, clas_weights=clas_weights)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics_detsym.hw_L,     gaf=gaf, clas_weights=clas_weights)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics_detsym.pConf_L,  gaf=gaf, clas_weights=clas_weights)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics_detsym.nConf_L,  gaf=gaf, conf_th=1)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics_detsym.clas_acc, gaf=gaf)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics_detsym.cent_d,   gaf=gaf)]\n",
    "    learn.metrics += [partial(anchors_loss_metrics_detsym.hw_r,     gaf=gaf)]\n",
    "    \n",
    "    return learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def get_tbCb(learn,log_dir):\n",
    "    tbCb = tensorboard_callback_detsym.TensorBoardCallback(\n",
    "                                   learn=learn,\n",
    "                                   log_dir=log_dir,\n",
    "                                   plot_net=False,\n",
    "                                   plot_loss=True,\n",
    "                                   metric_plots=[],\n",
    "                                   hyper_plots=['lr'],\n",
    "                                   hist_plots=['res_blocks.2.0.conv1.weight',\n",
    "                                               'neck_blocks.0.pwConv.weight'],\n",
    "                                   hist_iters=50)\n",
    "    return tbCb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "def fit(learn,epoch_len,epochs,lr,callbacks):\n",
    "    scheduling_train_detsym.fit_with_warmup_multiAnnealPlat(learn,\n",
    "                                    epoch_len=epoch_len,\n",
    "                                    num_epoch=epochs,\n",
    "\n",
    "                                    lr_start=lr/10,\n",
    "                                    lr_constant=lr,\n",
    "                                    warmup_iter=10,\n",
    "\n",
    "                                    monitor='train_smooth',\n",
    "                                    worseN_thres=5,\n",
    "                                    annealRate=10,\n",
    "                                    duration_thres=30,\n",
    "                                    annealIte=10,\n",
    "                                    phaseMaxN=3,\n",
    "                                    finetune_stop=1,\n",
    "                                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "#用之前的符号检测的模型裁剪一下去掉后面的两个预测层测试。\n",
    "TEST_DSMODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "# get databunch\n",
    "data = databunch.get_databunch(data_root = '../../detect_symbol/data/ds_20200429', bs=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# export\n",
    "# grid anchor functions\n",
    "gaf = anchors_loss_metrics.GridAnchor_Funcs(fig_hw = (776,776)\n",
    "                         , grids = [(49,49)]\n",
    "                         , device = device)\n",
    "gvs, avs = gaf.gvs, gaf.avs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "opts = [partial(optimizer_detsym.Adam, betas=(0.9,0.99))]\n",
    "\n",
    "lrs = [1e-3]\n",
    "\n",
    "checkpoints = [None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = get_learn_detectsym_17clas(data,gaf)\n",
    "learn.opt_func = partial(optimizer_detsym.Adam, betas=(0.9,0.99))\n",
    "\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# export\n",
    "if not TEST_DSMODEL:\n",
    "    multi_train(get_learn=partial(get_learn_detectsym_17clas,data=data,gaf=gaf), \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 试一下用之前的符号检测的resnet18的网络去掉后面两个预测层进行训练，也就是现在的这个病树检测的网络加上了宽高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化。一次性调用。\n",
    "if TEST_DSMODEL and False:\n",
    "    #先用detect_symbol的数据集测试\n",
    "    data = databunch.get_databunch(data_root = '../../detect_symbol/data/ds_20200429', bs=8, device=device)\n",
    "    x,y = data.one_batch()\n",
    "\n",
    "    model = get_resnet18_1ssd_dsmodel()\n",
    "    # check layer output stats before runtime init.\n",
    "    init_model_detsym.show_layer_stats(model,x)\n",
    "    # runtime init\n",
    "    init_model_detsym.runtime_init_linear(model, x, init_model_detsym.hook_init);\n",
    "    # check layer output stats after runtime init\n",
    "    init_model_detsym.show_layer_stats(model,x)\n",
    "    # 因为runtime_init耗时较长，保存初始化后的模型，方便快速调用\n",
    "    # torch.save(model.state_dict(), './models/ds_resnet18_ssd_init.pth') # 只在要保存初始化模型时运行该行代码\n",
    "    init_model_detsym.runtime_init_linear(model, x, init_model_detsym.hook_init);\n",
    "    torch.save(model.state_dict(), './models/ds_resnet18_1ssd_init.pth') # 只在要保存初始化模型时运行该行代码\n",
    "    pdict = torch.load('../../detect_symbol/models/pytorch_pretrained/resnet18-5c106cde.pth')\n",
    "    # 将模型中与 resnet18 对应的部分的参数从预训练模型加载\n",
    "    link_names = [(r'^conv1',        'conv1'),\n",
    "                   (r'^bn1',          'bn1'),\n",
    "                   (r'^res_blocks.0', 'layer1'),\n",
    "                   (r'^res_blocks.1', 'layer2'),\n",
    "                   (r'^res_blocks.2', 'layer3'),\n",
    "                   (r'^res_blocks.3', 'layer4')]\n",
    "    init_model_detsym.init_to_pretrained(model, pdict, link_names)\n",
    "    torch.save(model.state_dict(), './models/ds_pretrained_res18_1ssd.pth') # 只在要保存初始化模型时运行该行代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anchors_loss_metrics_detsym' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0f1114f4f461>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#gvs,_,_,avs,_,_ = nb_anchors_loss_metrics.get_ga666()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m gvs,_,_,avs,_,_ = anchors_loss_metrics_detsym.get_grids_anchors(fig_hw = (776,776),\n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mgrids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m49\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     anchors = [[(1, 1)]])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'anchors_loss_metrics_detsym' is not defined"
     ]
    }
   ],
   "source": [
    "# export\n",
    "#gvs,_,_,avs,_,_ = nb_anchors_loss_metrics.get_ga666()\n",
    "gvs,_,_,avs,_,_ = anchors_loss_metrics_detsym.get_grids_anchors(fig_hw = (776,776),\n",
    "                    grids = [(49,49)],\n",
    "                    anchors = [[(1, 1)]])\n",
    "gaf = anchors_loss_metrics_detsym.GridAnchor_Funcs(gvs,avs,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#试一下\n",
    "learn = get_learn_detectsym_17clas_dsmodel(data,gaf)\n",
    "learn.opt_func = partial(optimizer_detsym.Adam, betas=(0.9,0.99))\n",
    "\n",
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.recorder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "if TEST_DSMODEL:\n",
    "    print('TEST_DSMODEL!!!')\n",
    "    input('any key')\n",
    "    multi_train(get_learn=partial(get_learn_detectsym_17clas_dsmodel,data=data,gaf=gaf), \n",
    "            epoch_len=1e9, epochs=500,\n",
    "            opts=opts, lrs=lrs, checkpoints=checkpoints,\n",
    "            tb_log_root='./tb_log/',\n",
    "            autoSave=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted train_script_logger.ipynb to train_script_logger.py\r\n"
     ]
    }
   ],
   "source": [
    "!python ../notebook2script.py --fname 'train_script_logger.ipynb' --outputDir './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
